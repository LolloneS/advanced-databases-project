# Advanced Databases 2ECTS Project's Report
# Global Commodity Trade Statistics 
# Lorenzo Soligo - 806954

## Abstract
In this report I will compare the performances of a MongoDB database which uses some collections linked with references versus a MongoDB database using a one-collection embedded-document structure, whilst working with a huge dataset (~1GB, 8+ million rows) representing trades statistics for many countries, with and without indexes.

## Dataset
[Global Commodity Trade Statistics](https://www.kaggle.com/unitednations/global-commodity-trade-statistics) by [United Nations](https://www.kaggle.com/unitednations)

### Fields
The dataset fields are:
* `country_or_area`:
  * name of the country or area the trade refers to
* `year`:
  * year in which the trade was done
* `comm_code`
  * code of the commodity traded
* `commodity`
  * name of the commodity traded
* `flow`
  * flow of the trade: Import, Export, ...
* `trade_usd`
  * amount of the trade, in US dollars
* `weight_kg`
  * weight of the commodity traded, in kilograms
* `quantity_name`
  * type of quantity (e.g. number of items, weight in kilograms, ...)
* `quantity`
  * number representing the quantity (e.g. number of items = 5)
* `category`
  * category of the trade




## Single collection, embedded structure

### Modeling the dataset

Modeling the dataset requires thinking about the best way to embed the fields in order to achieve clarity, ease of use and good performance.

After some reasoning, I think this is one of the best choices that could be made for the dataset. 
Every document represents a single trade.

* `ID` (generated by Mongo)
* `country_or_area`
* `year`
* `commodity`
  - `name`
  - `code`
  - `category`
* `trade_details`
  * `flow`
  * `weight_kg`
  * `trade_usd`
  * `quantity`
  * `quantity_name`



#### Sample document

```js
{
    "_id" : ObjectId("5b06df5f0cea1a1aa46b4ce2"),
 	"country_or_area" : "Afghanistan",
 	"year" : "2016",
 	"commodity" : {
        "name" : "Sheep, live",
        "code" : "010410",
        "category" : "01_live_animals"
    },
 	"trade_details" : {
        "flow" : "Export",
        "weight_kg" : 2339,
        "trade_usd" : 6088,
        "quantity" : 51,
        "quantity_name" : "Number of items"
    }
}    
```





## Two collections with references

### Document structure (example)

Structuring the database using more than one collection lets us divide the *commodities* from the rest of the data regarding the trades, giving us the advantage of saving space - since many commodities are repeated thousands of times over the dataset's millions of rows

The rows will be thus splitted in two collections this way:

1. **trades**

* `ID` (generated by Mongo)
* `country_or_area`
* `year`
* `commodity`
* `trade_details`
  - `flow`
  - `weight_kg`
  - `trade_usd`
  - `quantity`
  - `quantity_name`

2. **commodities**

* `ID` (generated by Mongo)
* `name`
* `code`
* `category`



#### Sample document

**Trade**
```js
{
   "_id":ObjectId("5b081ed80cea1a1a64ca495b"),
   "country_or_area":"Afghanistan",
   "year":"2016",
   "comm_code":"010410",
   "trade_details":{
      "flow":"Export",
      "weight_kg":2339,
      "trade_usd":6088,
      "quantity":51,
      "quantity_name":"Number of items"
   }
}
```



**Commodity**

```js
{
   "_id":ObjectId("5b0820620cea1a1a6447cdab"),
   "name":"Oak or chestnut tanning extract",
   "code":"320130",
   "category":"32_tanning_dyeing_extracts_tannins_derivs_pigments_et"
}
```


## Time spent to parse and import the dataset

|    Single collection    | Two collections |
| :---------------------: | :-------------: |
|         ~350s           |     ~320s       |


## DB size once imported
|    Single collection    | Two collections |
| :---------------------: | :-------------: |
|        0.77GB           |     0.55GB      |



## Time spent to create the indexes

| Field \ Database  | Embedded | Reference |
| ----------------- | -------- | --------- |
| `country_or_area` | 24s      | 20s       |
| `commodity.name`  | 34s      | 0.02s     |
| `commodity.code`  | 25s      | 0.02s     |
| `year`            | 40s      | 26s       |



## Execution time for the selected queries

#### Laptop specs:
- Intel Core i5-6300U
- 8GB RAM DDR4
- 512GB NVMe SSD 




#### 1. Find for each year the country whose Export gain is highest
| Indexes \ DB        | Embedded | Two collections |
| ------------------- | :------: | :-------------: |
| **With Indexes**    |   9.9s   |       8.17s       |
| **Without Indexes** |   11.0s  |       13.2s       |



#### 2. For each year, find the country which traded more kilograms of *010511*

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |  0.12s   |       3.2s      |
| **Without Indexes** |  6.2s    |       3.8s      |



#### 3. Find the year in which more money was traded across all countries

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   12.0s    |       11.0s   |
| **Without Indexes** |   13.0s    |       15.1s   |



#### 4. Find out whether Canada traded more sheeps or goats alive

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   0.6s   |       0.7s      |
| **Without Indexes** |   5.0s   |       7.2s      |



#### 5. Find all the countries that have not traded *010600* or *010519* in 1998

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   2.5s   |      2.7s      |
| **Without Indexes** |   13.6s  |      12.5s     |



#### 6. Find the most expensive trade for every year and for each country 

|    Indexes \ DB     | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
|  **With Indexes**   |   15.6s   | More than 30 minutes|
| **Without Indexes** |   16.3s   | More than 30 minutes|



#### 7. Find out whether Italy traded more goats than Canada

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   0.1s   |      0.5s       |
| **Without Indexes** |   4.9s   |      3.9s       |



#### 8. Find all the categories

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   6.5s   |      0.07s     |
| **Without Indexes** |   8.3s   |      0.1s      |



## Conclusions
As we can see, the queries' execution times are similar given the two structures adopted.
The embedded structure usually performs slightly better because of the time saved by avoiding the use `lookup` to "join" documents taken from two different collections.
The structure using references, on the other hand, performs way better when only data about the commodities is analyzed, because 1) the commodities are only a few thousands and 2) the commodities' documents are pretty small. Also, queries in which filtering some commodities is required often perform better using the reference structure because the "preprocessing" (i.e. only keeping few commodities) greatly speeds up the lookup between the two collections. 
In one case (query #6) we can appreciate how well the embedded structure performs when lots of joins are required: this is a clear example of how important it is to choose the right document structure when modeling the dataset.



## Some extra stuff

I created some Python scripts in order to parse and import the dataset into MongoDB.

I also tried to automate the process of setting the database up, creating indexes and running the queries as much as possible. This also helped me in measuring the time taken by every single step.




