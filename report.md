# Advanced Databases 2ECTS Project's Report
# Global Commodity Trade Statistics 
# Lorenzo Soligo - 806954

## Abstract
In this report I will compare the performances of a MongoDB database which uses some collections linked with references versus a MongoDB database using a one-collection embedded-document structure, whilst working with a huge dataset (~1GB, 8+ million rows) representing trades statistics for many countries, with and without indexes.

## Dataset
[Global Commodity Trade Statistics](https://www.kaggle.com/unitednations/global-commodity-trade-statistics) by [United Nations](https://www.kaggle.com/unitednations).

I chose this dataset because it allows me to work with a huge amount of data in order to try and test how well the reference and the embedded structures perform while working with lots of data.
The size of the dataset also lets me clearly see how much indexes impact on performance.

### Fields
The dataset fields are:
* `country_or_area`:
  * name of the country or area the trade refers to
* `year`:
  * year in which the trade was done
* `comm_code`
  * code of the commodity traded
* `commodity`
  * name of the commodity traded
* `flow`
  * flow of the trade: Import, Export, ...
* `trade_usd`
  * amount of the trade, in US dollars
* `weight_kg`
  * weight of the commodity traded, in kilograms
* `quantity_name`
  * type of quantity (e.g. number of items, weight in kilograms, ...)
* `quantity`
  * number representing the quantity (e.g. number of items = 5)
* `category`
  * category of the trade




## Single collection, embedded structure

### Modeling the dataset

Modeling the dataset requires thinking about the best way to embed the fields in order to achieve clarity, ease of use and good performance.

After some reasoning, I think this is one of the best choices that could be made for the dataset. 
Every document represents a single trade.

* `ID` (generated by Mongo)
* `country_or_area`
* `year`
* `commodity`
  - `name`
  - `code`
  - `category`
* `trade_details`
  * `flow`
  * `weight_kg`
  * `trade_usd`
  * `quantity`
  * `quantity_name`



#### Sample document

```js
{
    "_id" : ObjectId("5b06df5f0cea1a1aa46b4ce2"),
 	"country_or_area" : "Afghanistan",
 	"year" : "2016",
 	"commodity" : {
        "name" : "Sheep, live",
        "code" : "010410",
        "category" : "01_live_animals"
    },
 	"trade_details" : {
        "flow" : "Export",
        "weight_kg" : 2339,
        "trade_usd" : 6088,
        "quantity" : 51,
        "quantity_name" : "Number of items"
    }
}    
```





## Two collections with references

### Document structure (example)

Structuring the database using more than one collection lets us divide the *commodities* from the rest of the data regarding the trades, giving us the advantage of saving space - since many commodities are repeated thousands of times over the dataset's millions of rows

The rows of the dataset will be thus splitted in two collections this way:

1. **trades**

* `ID` (generated by Mongo)
* `country_or_area`
* `year`
* `commodity`
* `trade_details`
  - `flow`
  - `weight_kg`
  - `trade_usd`
  - `quantity`
  - `quantity_name`

2. **commodities**

* `ID` (generated by Mongo)
* `name`
* `code`
* `category`



#### Sample document

**Trade**
```js
{
   "_id":ObjectId("5b081ed80cea1a1a64ca495b"),
   "country_or_area":"Afghanistan",
   "year":"2016",
   "comm_code":"010410",
   "trade_details":{
      "flow":"Export",
      "weight_kg":2339,
      "trade_usd":6088,
      "quantity":51,
      "quantity_name":"Number of items"
   }
}
```



**Commodity**

```js
{
   "_id":ObjectId("5b0820620cea1a1a6447cdab"),
   "name":"Oak or chestnut tanning extract",
   "code":"320130",
   "category":"32_tanning_dyeing_extracts_tannins_derivs_pigments_et"
}
```


## Time spent to parse and import the dataset

|    Single collection    | Two collections |
| :---------------------: | :-------------: |
|         ~234s           |     ~219s       |


## DB size once imported
|    Single collection    | Two collections |
| :---------------------: | :-------------: |
|        0.77GB           |     0.55GB      |



## Time spent to create the indexes

| Field \ Database  |  Embedded  |   Reference  |
| ----------------- | ---------- | ------------ |
| `country_or_area` | 16.9s      | 16.8s        |
| `commodity.name`  | 25.2s      | 0.02s        |
| `commodity.code`  | 18.5s      | 17.9s + 0.02s|
| `year`            | 19.0s      | 18.5s        |



## Execution time for the selected queries

#### Laptop specs:
- Intel Core i5-6300U
- 8GB RAM DDR4
- 512GB NVMe SSD 



#### 1. Find for each year the country whose Export gain is highest
| Indexes \ DB        | Embedded | Two collections |
| ------------------- | :------: | :-------------: |
| **With Indexes**    |   7.0s   |       6.8s       |
| **Without Indexes** |   7.2s   |       7.6s       |

The query is the same on the two collections, thus the difference is insignificant.


#### 2. For each year, find the country which traded more kilograms of *010511*

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |  0.1s    |       0.1s      |
| **Without Indexes** |  3.7s    |       3.3s      |

The query is the same on the two collections, thus the difference is insignificant.



#### 3. Find the year in which more money was traded across all countries

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   8.9s    |       9.0s   |
| **Without Indexes** |   9.0s    |       9.0s   |

The query is the same on the two collections, thus the difference is insignificant.



#### 4. Find out whether Canada traded more sheeps or goats alive

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   0.4s   |       0.09s      |
| **Without Indexes** |   3.4s   |       6.0s      |

The query without indexes is faster in the embedded structure because there is no need for a join (`lookup`) which takes a lot of time if there are no indexes. On the other hand, the query with indexes runs faster in the reference structure because the documents are smaller and few joins are required.


#### 5. Find all the countries that have not traded *010600* or *010519* in 1998

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   2.0s   |      2.4s      |
| **Without Indexes** |   8.5s  |      10.3s     |

The query runs slightly faster in the embedded structure because of the time saved by not joining two collections.


#### 6. Find the most expensive trade for every year and for each country 

|    Indexes \ DB     | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
|  **With Indexes**   |   11.3s   | More than 30 minutes|
| **Without Indexes** |   11.2s   | More than 30 minutes|

The query in the reference structure takes more than 30 minutes (more than 1 hour, actually) because of all the time wasted joining and grouping the two collections. On the other hand, in the embedded structure there are no joins to be made and the query only requires a linear scan, thus the time required is low. Indexes don't improve the situation since the vast majority of the time is spent scanning files (which are made heavier by the indexes). The only way to get the answer is to scan all files.


#### 7. Find out whether Italy traded more goats than Canada

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   0.1s   |      0.06s       |
| **Without Indexes** |   3.7s   |      2.9s       |

The query is faster in the reference structure because the documents to be scanned are smaller and they are pre-filtered. Anyway, the time wasted by joining the two collections makes the difference less significant.


#### 8. Find all the categories

| Indexes \ DB        | Embedded | Two collections |
| :-----------------: | :------: | :-------------: |
| **With Indexes**    |   4.8s   |      0.06s     |
| **Without Indexes** |   4.9s   |      0.07s      |

The query is way faster in the reference structure because it contains few documents which are also very small.




## Conclusions
As we can see, the queries' execution times are similar given the two structures adopted.
The embedded structure usually performs slightly better because of the time saved by avoiding the use `lookup` to "join" documents taken from two different collections.

The structure using references, on the other hand, performs better when only data about the commodities is analyzed, because 1) the commodities are only a few thousands and 2) the commodities' documents are pretty small. Also, queries in which filtering some commodities is required often perform better using the reference structure because the "pre-filtering" (i.e. only keeping few commodities) greatly speeds up the lookup between the two collections. 

In one case (query #6) we can appreciate how well the embedded structure performs when lots of joins are required: this is a clear example of how important it is to choose the right document structure when modeling the dataset. The query couldn't complete in more than one hour using the reference structure, while it took less than 15 seconds with the embedded one!



## Some extra stuff
I created some Python scripts in order to parse and import the dataset into MongoDB.

I also tried to automate the process of setting the database up, creating indexes and running the queries as much as possible. This also helped me in measuring the time taken by every single step.




